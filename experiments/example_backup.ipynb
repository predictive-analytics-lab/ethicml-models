{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethicml\n",
    "from ethicml.algorithms.inprocess import GPyT, GPyTDemPar, GPyTEqOdds, LR, SVM, Agarwal, Kamiran, Kamishima, LR\n",
    "from ethicml.evaluators import evaluate_models, CrossValidator, run_metrics\n",
    "from ethicml.data import Compas, Adult, load_data\n",
    "from ethicml.metrics import Accuracy, ProbPos, TPR, TNR, AbsCV\n",
    "from ethicml.preprocessing import train_test_split\n",
    "from ethicml.visualisation.plot import plot_mean_std_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T HAVE TO UNDERSTAND THE CODE IN THIS CELL\n",
    "# we only tell the model where the \"run.py\" is and where the python executable is\n",
    "file_name = Path('..') / \"run.py\"\n",
    "def gp(**kwargs):\n",
    "    return GPyT(file_name=file_name, executable=sys.executable, **kwargs)\n",
    "def gp_dp(**kwargs):\n",
    "    return GPyTDemPar(file_name=file_name, executable=sys.executable, **kwargs)\n",
    "def gp_eo(**kwargs):\n",
    "    return GPyTEqOdds(file_name=file_name, executable=sys.executable, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnr_race_False = 0.724\n",
    "# tnr_race_True = 0.702\n",
    "# tnr_sex_True = 0.724\n",
    "# tnr_sex_False = 0.744\n",
    "tnr_in_true_race = 0.71\n",
    "tnr_in_false_race = 0.74\n",
    "tnr_in_true_sex = 0.72\n",
    "tnr_in_false_sex = 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = []\n",
    "\n",
    "algos += [gp(epochs=70, s_as_input=True)]\n",
    "algos += [gp(epochs=70, s_as_input=False)]\n",
    "# algos += [gp_dp(epochs=70, s_as_input=True)]\n",
    "for tpr in [0.7]:\n",
    "    algos += [gp_eo(epochs=70, s_as_input=True, tnr1=tnr_in_true_race, tnr0=tnr_in_true_race, tpr0=tpr, tpr1=tpr)]\n",
    "    algos += [gp_eo(epochs=70, s_as_input=False, tnr1=tnr_in_false_race, tnr0=tnr_in_false_race, tpr0=tpr, tpr1=tpr)]\n",
    "\n",
    "baselines = [\n",
    "    LR(),\n",
    "    SVM(),\n",
    "    Agarwal(fairness=\"EqOd\"),\n",
    "    Kamiran(),\n",
    "#     Kamishima(),\n",
    "]\n",
    "algos += baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    Compas(\"Race\"),\n",
    "#     Compas(\"Sex\"),\n",
    "#     Adult(\"Race\"),\n",
    "#     Adult(\"Sex\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [01:33<07:29, 13.22s/it, model=Agarwal LR, dataset=Compas Race, transform=no_transform, repeat=0]                                                 /mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:358: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not weights.index.contains(h_idx):\n",
      " 35%|███▌      | 14/40 [03:17<05:05, 11.74s/it, model=Agarwal LR, dataset=Compas Race, transform=no_transform, repeat=1]                                                 /mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:358: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not weights.index.contains(h_idx):\n",
      " 55%|█████▌    | 22/40 [05:01<03:29, 11.65s/it, model=Agarwal LR, dataset=Compas Race, transform=no_transform, repeat=2]                                                 /mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:358: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not weights.index.contains(h_idx):\n",
      " 75%|███████▌  | 30/40 [06:43<01:55, 11.57s/it, model=Agarwal LR, dataset=Compas Race, transform=no_transform, repeat=3]                                                 /mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:358: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not weights.index.contains(h_idx):\n",
      " 95%|█████████▌| 38/40 [08:27<00:23, 11.61s/it, model=Agarwal LR, dataset=Compas Race, transform=no_transform, repeat=4]                                                 /mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:309: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not Qsum.index.contains(h_idx):\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/fairlearn/classred.py:358: FutureWarning: The 'contains' method is deprecated and will be removed in a future version. Use 'key in index' instead of 'index.contains(key)'\n",
      "  if not weights.index.contains(h_idx):\n",
      "100%|██████████| 40/40 [08:37<00:00,  7.93s/it, model=Kamiran & Calders LR, dataset=Compas Race, transform=no_transform, repeat=4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TNR</th>\n",
       "      <th>TNR_race_0</th>\n",
       "      <th>TNR_race_0-race_1</th>\n",
       "      <th>TNR_race_0/race_1</th>\n",
       "      <th>TNR_race_1</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TPR_race_0</th>\n",
       "      <th>TPR_race_0-race_1</th>\n",
       "      <th>TPR_race_0/race_1</th>\n",
       "      <th>TPR_race_1</th>\n",
       "      <th>prob_pos</th>\n",
       "      <th>prob_pos_race_0</th>\n",
       "      <th>prob_pos_race_0-race_1</th>\n",
       "      <th>prob_pos_race_0/race_1</th>\n",
       "      <th>prob_pos_race_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>transform</th>\n",
       "      <th>model</th>\n",
       "      <th>repeat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"40\" valign=\"top\">Compas Race</th>\n",
       "      <th rowspan=\"40\" valign=\"top\">no_transform</th>\n",
       "      <th>GPyT_in_True</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.679092</td>\n",
       "      <td>0.719068</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.145537</td>\n",
       "      <td>0.819748</td>\n",
       "      <td>0.807407</td>\n",
       "      <td>0.628885</td>\n",
       "      <td>0.676166</td>\n",
       "      <td>0.160638</td>\n",
       "      <td>0.762428</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.435170</td>\n",
       "      <td>0.500623</td>\n",
       "      <td>0.187398</td>\n",
       "      <td>0.625671</td>\n",
       "      <td>0.313225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_False</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.679903</td>\n",
       "      <td>0.732169</td>\n",
       "      <td>0.693046</td>\n",
       "      <td>0.099547</td>\n",
       "      <td>0.874403</td>\n",
       "      <td>0.792593</td>\n",
       "      <td>0.614260</td>\n",
       "      <td>0.645078</td>\n",
       "      <td>0.104705</td>\n",
       "      <td>0.837686</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.421394</td>\n",
       "      <td>0.469489</td>\n",
       "      <td>0.137703</td>\n",
       "      <td>0.706697</td>\n",
       "      <td>0.331787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.670178</td>\n",
       "      <td>0.668122</td>\n",
       "      <td>0.649880</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.933338</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.672761</td>\n",
       "      <td>0.678756</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.969987</td>\n",
       "      <td>0.658385</td>\n",
       "      <td>0.482982</td>\n",
       "      <td>0.508095</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.436195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.682334</td>\n",
       "      <td>0.711790</td>\n",
       "      <td>0.671463</td>\n",
       "      <td>0.102611</td>\n",
       "      <td>0.867440</td>\n",
       "      <td>0.774074</td>\n",
       "      <td>0.645338</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.131143</td>\n",
       "      <td>0.808253</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.446515</td>\n",
       "      <td>0.499377</td>\n",
       "      <td>0.151349</td>\n",
       "      <td>0.696924</td>\n",
       "      <td>0.348028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.675851</td>\n",
       "      <td>0.743814</td>\n",
       "      <td>0.697842</td>\n",
       "      <td>0.116973</td>\n",
       "      <td>0.856442</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.590494</td>\n",
       "      <td>0.626943</td>\n",
       "      <td>0.123837</td>\n",
       "      <td>0.802474</td>\n",
       "      <td>0.503106</td>\n",
       "      <td>0.404376</td>\n",
       "      <td>0.458281</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.663226</td>\n",
       "      <td>0.303944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.687196</td>\n",
       "      <td>0.788937</td>\n",
       "      <td>0.752998</td>\n",
       "      <td>0.091447</td>\n",
       "      <td>0.891708</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.559415</td>\n",
       "      <td>0.608808</td>\n",
       "      <td>0.167815</td>\n",
       "      <td>0.724356</td>\n",
       "      <td>0.440994</td>\n",
       "      <td>0.365478</td>\n",
       "      <td>0.420922</td>\n",
       "      <td>0.158741</td>\n",
       "      <td>0.622874</td>\n",
       "      <td>0.262181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agarwal LR</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.761281</td>\n",
       "      <td>0.724221</td>\n",
       "      <td>0.094298</td>\n",
       "      <td>0.884794</td>\n",
       "      <td>0.818519</td>\n",
       "      <td>0.561243</td>\n",
       "      <td>0.598446</td>\n",
       "      <td>0.126396</td>\n",
       "      <td>0.788793</td>\n",
       "      <td>0.472050</td>\n",
       "      <td>0.381686</td>\n",
       "      <td>0.430884</td>\n",
       "      <td>0.140861</td>\n",
       "      <td>0.673089</td>\n",
       "      <td>0.290023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamiran &amp; Calders LR</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.676661</td>\n",
       "      <td>0.756914</td>\n",
       "      <td>0.717026</td>\n",
       "      <td>0.101492</td>\n",
       "      <td>0.876005</td>\n",
       "      <td>0.818519</td>\n",
       "      <td>0.575868</td>\n",
       "      <td>0.608808</td>\n",
       "      <td>0.111914</td>\n",
       "      <td>0.816175</td>\n",
       "      <td>0.496894</td>\n",
       "      <td>0.390600</td>\n",
       "      <td>0.439601</td>\n",
       "      <td>0.140298</td>\n",
       "      <td>0.680853</td>\n",
       "      <td>0.299304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_True</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.687196</td>\n",
       "      <td>0.775076</td>\n",
       "      <td>0.721550</td>\n",
       "      <td>0.143756</td>\n",
       "      <td>0.833866</td>\n",
       "      <td>0.865306</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.641361</td>\n",
       "      <td>0.161980</td>\n",
       "      <td>0.747444</td>\n",
       "      <td>0.479381</td>\n",
       "      <td>0.393841</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.165814</td>\n",
       "      <td>0.633827</td>\n",
       "      <td>0.287016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_False</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.692869</td>\n",
       "      <td>0.734043</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.105085</td>\n",
       "      <td>0.868644</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.685864</td>\n",
       "      <td>0.118854</td>\n",
       "      <td>0.826710</td>\n",
       "      <td>0.567010</td>\n",
       "      <td>0.443274</td>\n",
       "      <td>0.488050</td>\n",
       "      <td>0.125864</td>\n",
       "      <td>0.742110</td>\n",
       "      <td>0.362187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.677472</td>\n",
       "      <td>0.688450</td>\n",
       "      <td>0.673123</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>0.942373</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.664931</td>\n",
       "      <td>0.680628</td>\n",
       "      <td>0.046608</td>\n",
       "      <td>0.931523</td>\n",
       "      <td>0.634021</td>\n",
       "      <td>0.476499</td>\n",
       "      <td>0.496855</td>\n",
       "      <td>0.057220</td>\n",
       "      <td>0.884836</td>\n",
       "      <td>0.439636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.683955</td>\n",
       "      <td>0.699088</td>\n",
       "      <td>0.653753</td>\n",
       "      <td>0.121757</td>\n",
       "      <td>0.842997</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714660</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>0.800612</td>\n",
       "      <td>0.572165</td>\n",
       "      <td>0.471637</td>\n",
       "      <td>0.523270</td>\n",
       "      <td>0.145138</td>\n",
       "      <td>0.722632</td>\n",
       "      <td>0.378132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.682334</td>\n",
       "      <td>0.775076</td>\n",
       "      <td>0.748184</td>\n",
       "      <td>0.072224</td>\n",
       "      <td>0.911966</td>\n",
       "      <td>0.820408</td>\n",
       "      <td>0.576389</td>\n",
       "      <td>0.617801</td>\n",
       "      <td>0.122956</td>\n",
       "      <td>0.800979</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.388979</td>\n",
       "      <td>0.427673</td>\n",
       "      <td>0.108766</td>\n",
       "      <td>0.745679</td>\n",
       "      <td>0.318907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.681524</td>\n",
       "      <td>0.800912</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.102594</td>\n",
       "      <td>0.881436</td>\n",
       "      <td>0.865306</td>\n",
       "      <td>0.545139</td>\n",
       "      <td>0.591623</td>\n",
       "      <td>0.138015</td>\n",
       "      <td>0.766718</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.360616</td>\n",
       "      <td>0.407547</td>\n",
       "      <td>0.131921</td>\n",
       "      <td>0.676306</td>\n",
       "      <td>0.275626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agarwal LR</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.654781</td>\n",
       "      <td>0.761398</td>\n",
       "      <td>0.753027</td>\n",
       "      <td>0.022484</td>\n",
       "      <td>0.971008</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.532986</td>\n",
       "      <td>0.565445</td>\n",
       "      <td>0.096373</td>\n",
       "      <td>0.829563</td>\n",
       "      <td>0.469072</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.067426</td>\n",
       "      <td>0.831435</td>\n",
       "      <td>0.332574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamiran &amp; Calders LR</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.681524</td>\n",
       "      <td>0.784195</td>\n",
       "      <td>0.760291</td>\n",
       "      <td>0.064199</td>\n",
       "      <td>0.922135</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.564236</td>\n",
       "      <td>0.604712</td>\n",
       "      <td>0.120176</td>\n",
       "      <td>0.801267</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.378444</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.103021</td>\n",
       "      <td>0.751812</td>\n",
       "      <td>0.312073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_True</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.683144</td>\n",
       "      <td>0.745840</td>\n",
       "      <td>0.689573</td>\n",
       "      <td>0.155615</td>\n",
       "      <td>0.815881</td>\n",
       "      <td>0.845188</td>\n",
       "      <td>0.610820</td>\n",
       "      <td>0.683417</td>\n",
       "      <td>0.237703</td>\n",
       "      <td>0.652185</td>\n",
       "      <td>0.445714</td>\n",
       "      <td>0.419773</td>\n",
       "      <td>0.491463</td>\n",
       "      <td>0.213686</td>\n",
       "      <td>0.565205</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_False</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.687196</td>\n",
       "      <td>0.742814</td>\n",
       "      <td>0.706161</td>\n",
       "      <td>0.101370</td>\n",
       "      <td>0.874469</td>\n",
       "      <td>0.807531</td>\n",
       "      <td>0.623037</td>\n",
       "      <td>0.675879</td>\n",
       "      <td>0.173022</td>\n",
       "      <td>0.744004</td>\n",
       "      <td>0.502857</td>\n",
       "      <td>0.427066</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.155597</td>\n",
       "      <td>0.675345</td>\n",
       "      <td>0.323671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.679903</td>\n",
       "      <td>0.700454</td>\n",
       "      <td>0.675355</td>\n",
       "      <td>0.069414</td>\n",
       "      <td>0.906797</td>\n",
       "      <td>0.744770</td>\n",
       "      <td>0.656195</td>\n",
       "      <td>0.693467</td>\n",
       "      <td>0.122039</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.465154</td>\n",
       "      <td>0.503659</td>\n",
       "      <td>0.114770</td>\n",
       "      <td>0.772128</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.674230</td>\n",
       "      <td>0.757943</td>\n",
       "      <td>0.715640</td>\n",
       "      <td>0.116996</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>0.832636</td>\n",
       "      <td>0.577661</td>\n",
       "      <td>0.630653</td>\n",
       "      <td>0.173510</td>\n",
       "      <td>0.724872</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.397893</td>\n",
       "      <td>0.452439</td>\n",
       "      <td>0.162584</td>\n",
       "      <td>0.640650</td>\n",
       "      <td>0.289855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.668558</td>\n",
       "      <td>0.750378</td>\n",
       "      <td>0.722749</td>\n",
       "      <td>0.076414</td>\n",
       "      <td>0.904382</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.574171</td>\n",
       "      <td>0.635678</td>\n",
       "      <td>0.201393</td>\n",
       "      <td>0.683185</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.400324</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.151703</td>\n",
       "      <td>0.663794</td>\n",
       "      <td>0.299517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.673420</td>\n",
       "      <td>0.783661</td>\n",
       "      <td>0.736967</td>\n",
       "      <td>0.129142</td>\n",
       "      <td>0.850894</td>\n",
       "      <td>0.866109</td>\n",
       "      <td>0.546248</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.185872</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>0.417143</td>\n",
       "      <td>0.369530</td>\n",
       "      <td>0.428049</td>\n",
       "      <td>0.174426</td>\n",
       "      <td>0.592510</td>\n",
       "      <td>0.253623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agarwal LR</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.667747</td>\n",
       "      <td>0.770045</td>\n",
       "      <td>0.755924</td>\n",
       "      <td>0.039055</td>\n",
       "      <td>0.950873</td>\n",
       "      <td>0.794979</td>\n",
       "      <td>0.549738</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.190897</td>\n",
       "      <td>0.686045</td>\n",
       "      <td>0.417143</td>\n",
       "      <td>0.378444</td>\n",
       "      <td>0.420732</td>\n",
       "      <td>0.126046</td>\n",
       "      <td>0.700413</td>\n",
       "      <td>0.294686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamiran &amp; Calders LR</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.665316</td>\n",
       "      <td>0.759455</td>\n",
       "      <td>0.736967</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>0.922173</td>\n",
       "      <td>0.799163</td>\n",
       "      <td>0.556719</td>\n",
       "      <td>0.615578</td>\n",
       "      <td>0.192721</td>\n",
       "      <td>0.686927</td>\n",
       "      <td>0.422857</td>\n",
       "      <td>0.387358</td>\n",
       "      <td>0.434146</td>\n",
       "      <td>0.139460</td>\n",
       "      <td>0.678771</td>\n",
       "      <td>0.294686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_True</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.700972</td>\n",
       "      <td>0.753446</td>\n",
       "      <td>0.688564</td>\n",
       "      <td>0.175072</td>\n",
       "      <td>0.797285</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.641997</td>\n",
       "      <td>0.719340</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.602109</td>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.432739</td>\n",
       "      <td>0.518563</td>\n",
       "      <td>0.265430</td>\n",
       "      <td>0.488143</td>\n",
       "      <td>0.253133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_False</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.698541</td>\n",
       "      <td>0.791730</td>\n",
       "      <td>0.749392</td>\n",
       "      <td>0.114245</td>\n",
       "      <td>0.867717</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.593804</td>\n",
       "      <td>0.658019</td>\n",
       "      <td>0.237637</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.420382</td>\n",
       "      <td>0.389789</td>\n",
       "      <td>0.457485</td>\n",
       "      <td>0.209365</td>\n",
       "      <td>0.542357</td>\n",
       "      <td>0.248120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.690438</td>\n",
       "      <td>0.690658</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.064738</td>\n",
       "      <td>0.911488</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>0.690189</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.168970</td>\n",
       "      <td>0.770374</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.488655</td>\n",
       "      <td>0.537725</td>\n",
       "      <td>0.151760</td>\n",
       "      <td>0.717774</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.696921</td>\n",
       "      <td>0.771822</td>\n",
       "      <td>0.720195</td>\n",
       "      <td>0.139309</td>\n",
       "      <td>0.837919</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.612737</td>\n",
       "      <td>0.676887</td>\n",
       "      <td>0.237396</td>\n",
       "      <td>0.649282</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.409238</td>\n",
       "      <td>0.481437</td>\n",
       "      <td>0.223292</td>\n",
       "      <td>0.536197</td>\n",
       "      <td>0.258145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.696921</td>\n",
       "      <td>0.781011</td>\n",
       "      <td>0.749392</td>\n",
       "      <td>0.085319</td>\n",
       "      <td>0.897786</td>\n",
       "      <td>0.834711</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.214517</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.445860</td>\n",
       "      <td>0.399514</td>\n",
       "      <td>0.458683</td>\n",
       "      <td>0.182993</td>\n",
       "      <td>0.601046</td>\n",
       "      <td>0.275689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.689627</td>\n",
       "      <td>0.790199</td>\n",
       "      <td>0.742092</td>\n",
       "      <td>0.129808</td>\n",
       "      <td>0.851120</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.576592</td>\n",
       "      <td>0.646226</td>\n",
       "      <td>0.257691</td>\n",
       "      <td>0.601237</td>\n",
       "      <td>0.388535</td>\n",
       "      <td>0.382496</td>\n",
       "      <td>0.455090</td>\n",
       "      <td>0.224513</td>\n",
       "      <td>0.506661</td>\n",
       "      <td>0.230576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agarwal LR</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.685575</td>\n",
       "      <td>0.785605</td>\n",
       "      <td>0.751825</td>\n",
       "      <td>0.091150</td>\n",
       "      <td>0.891871</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.573150</td>\n",
       "      <td>0.629717</td>\n",
       "      <td>0.209335</td>\n",
       "      <td>0.667573</td>\n",
       "      <td>0.420382</td>\n",
       "      <td>0.383306</td>\n",
       "      <td>0.441916</td>\n",
       "      <td>0.181265</td>\n",
       "      <td>0.589821</td>\n",
       "      <td>0.260652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamiran &amp; Calders LR</th>\n",
       "      <th>3-9640</th>\n",
       "      <td>0.688006</td>\n",
       "      <td>0.782542</td>\n",
       "      <td>0.749392</td>\n",
       "      <td>0.089451</td>\n",
       "      <td>0.893364</td>\n",
       "      <td>0.838843</td>\n",
       "      <td>0.581756</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.203671</td>\n",
       "      <td>0.680160</td>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.388979</td>\n",
       "      <td>0.446707</td>\n",
       "      <td>0.178536</td>\n",
       "      <td>0.600328</td>\n",
       "      <td>0.268170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_True</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.692058</td>\n",
       "      <td>0.736311</td>\n",
       "      <td>0.694712</td>\n",
       "      <td>0.103850</td>\n",
       "      <td>0.869954</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>0.635185</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.139686</td>\n",
       "      <td>0.792588</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.426256</td>\n",
       "      <td>0.483911</td>\n",
       "      <td>0.167009</td>\n",
       "      <td>0.654876</td>\n",
       "      <td>0.316901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_in_False</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.697731</td>\n",
       "      <td>0.744957</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.083426</td>\n",
       "      <td>0.895057</td>\n",
       "      <td>0.794964</td>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.132929</td>\n",
       "      <td>0.802621</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.422204</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.321596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.684765</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.713942</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.997602</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.647959</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.998937</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.444895</td>\n",
       "      <td>0.461634</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.894964</td>\n",
       "      <td>0.413146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_0.7_1tpr_0.7</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.700162</td>\n",
       "      <td>0.779539</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.109747</td>\n",
       "      <td>0.870172</td>\n",
       "      <td>0.845324</td>\n",
       "      <td>0.598148</td>\n",
       "      <td>0.630102</td>\n",
       "      <td>0.116589</td>\n",
       "      <td>0.814969</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.385737</td>\n",
       "      <td>0.441832</td>\n",
       "      <td>0.162489</td>\n",
       "      <td>0.632238</td>\n",
       "      <td>0.279343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.678282</td>\n",
       "      <td>0.757925</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.067792</td>\n",
       "      <td>0.915107</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>0.575926</td>\n",
       "      <td>0.619898</td>\n",
       "      <td>0.160438</td>\n",
       "      <td>0.741186</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.388169</td>\n",
       "      <td>0.439356</td>\n",
       "      <td>0.148277</td>\n",
       "      <td>0.662514</td>\n",
       "      <td>0.291080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.690438</td>\n",
       "      <td>0.788184</td>\n",
       "      <td>0.745192</td>\n",
       "      <td>0.107326</td>\n",
       "      <td>0.874107</td>\n",
       "      <td>0.852518</td>\n",
       "      <td>0.564815</td>\n",
       "      <td>0.599490</td>\n",
       "      <td>0.126517</td>\n",
       "      <td>0.788959</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.366288</td>\n",
       "      <td>0.422030</td>\n",
       "      <td>0.161466</td>\n",
       "      <td>0.617405</td>\n",
       "      <td>0.260563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agarwal LR</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.674230</td>\n",
       "      <td>0.756484</td>\n",
       "      <td>0.737981</td>\n",
       "      <td>0.046192</td>\n",
       "      <td>0.941095</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.568519</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.140927</td>\n",
       "      <td>0.767886</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>0.385737</td>\n",
       "      <td>0.429455</td>\n",
       "      <td>0.126639</td>\n",
       "      <td>0.705118</td>\n",
       "      <td>0.302817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kamiran &amp; Calders LR</th>\n",
       "      <th>4-12050</th>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.759366</td>\n",
       "      <td>0.733173</td>\n",
       "      <td>0.065388</td>\n",
       "      <td>0.918118</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>0.751947</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.380875</td>\n",
       "      <td>0.429455</td>\n",
       "      <td>0.140723</td>\n",
       "      <td>0.672322</td>\n",
       "      <td>0.288732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Accuracy  \\\n",
       "dataset     transform    model                                              repeat              \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410   0.679092   \n",
       "                         GPyT_in_False                                      0-2410   0.679903   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410   0.670178   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410   0.682334   \n",
       "                         Logistic Regression                                0-2410   0.675851   \n",
       "                         SVM                                                0-2410   0.687196   \n",
       "                         Agarwal LR                                         0-2410   0.672609   \n",
       "                         Kamiran & Calders LR                               0-2410   0.676661   \n",
       "                         GPyT_in_True                                       1-4820   0.687196   \n",
       "                         GPyT_in_False                                      1-4820   0.692869   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820   0.677472   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820   0.683955   \n",
       "                         Logistic Regression                                1-4820   0.682334   \n",
       "                         SVM                                                1-4820   0.681524   \n",
       "                         Agarwal LR                                         1-4820   0.654781   \n",
       "                         Kamiran & Calders LR                               1-4820   0.681524   \n",
       "                         GPyT_in_True                                       2-7230   0.683144   \n",
       "                         GPyT_in_False                                      2-7230   0.687196   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230   0.679903   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230   0.674230   \n",
       "                         Logistic Regression                                2-7230   0.668558   \n",
       "                         SVM                                                2-7230   0.673420   \n",
       "                         Agarwal LR                                         2-7230   0.667747   \n",
       "                         Kamiran & Calders LR                               2-7230   0.665316   \n",
       "                         GPyT_in_True                                       3-9640   0.700972   \n",
       "                         GPyT_in_False                                      3-9640   0.698541   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640   0.690438   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640   0.696921   \n",
       "                         Logistic Regression                                3-9640   0.696921   \n",
       "                         SVM                                                3-9640   0.689627   \n",
       "                         Agarwal LR                                         3-9640   0.685575   \n",
       "                         Kamiran & Calders LR                               3-9640   0.688006   \n",
       "                         GPyT_in_True                                       4-12050  0.692058   \n",
       "                         GPyT_in_False                                      4-12050  0.697731   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050  0.684765   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050  0.700162   \n",
       "                         Logistic Regression                                4-12050  0.678282   \n",
       "                         SVM                                                4-12050  0.690438   \n",
       "                         Agarwal LR                                         4-12050  0.674230   \n",
       "                         Kamiran & Calders LR                               4-12050  0.672609   \n",
       "\n",
       "                                                                                          TNR  \\\n",
       "dataset     transform    model                                              repeat              \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410   0.719068   \n",
       "                         GPyT_in_False                                      0-2410   0.732169   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410   0.668122   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410   0.711790   \n",
       "                         Logistic Regression                                0-2410   0.743814   \n",
       "                         SVM                                                0-2410   0.788937   \n",
       "                         Agarwal LR                                         0-2410   0.761281   \n",
       "                         Kamiran & Calders LR                               0-2410   0.756914   \n",
       "                         GPyT_in_True                                       1-4820   0.775076   \n",
       "                         GPyT_in_False                                      1-4820   0.734043   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820   0.688450   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820   0.699088   \n",
       "                         Logistic Regression                                1-4820   0.775076   \n",
       "                         SVM                                                1-4820   0.800912   \n",
       "                         Agarwal LR                                         1-4820   0.761398   \n",
       "                         Kamiran & Calders LR                               1-4820   0.784195   \n",
       "                         GPyT_in_True                                       2-7230   0.745840   \n",
       "                         GPyT_in_False                                      2-7230   0.742814   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230   0.700454   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230   0.757943   \n",
       "                         Logistic Regression                                2-7230   0.750378   \n",
       "                         SVM                                                2-7230   0.783661   \n",
       "                         Agarwal LR                                         2-7230   0.770045   \n",
       "                         Kamiran & Calders LR                               2-7230   0.759455   \n",
       "                         GPyT_in_True                                       3-9640   0.753446   \n",
       "                         GPyT_in_False                                      3-9640   0.791730   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640   0.690658   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640   0.771822   \n",
       "                         Logistic Regression                                3-9640   0.781011   \n",
       "                         SVM                                                3-9640   0.790199   \n",
       "                         Agarwal LR                                         3-9640   0.785605   \n",
       "                         Kamiran & Calders LR                               3-9640   0.782542   \n",
       "                         GPyT_in_True                                       4-12050  0.736311   \n",
       "                         GPyT_in_False                                      4-12050  0.744957   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050  0.713256   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050  0.779539   \n",
       "                         Logistic Regression                                4-12050  0.757925   \n",
       "                         SVM                                                4-12050  0.788184   \n",
       "                         Agarwal LR                                         4-12050  0.756484   \n",
       "                         Kamiran & Calders LR                               4-12050  0.759366   \n",
       "\n",
       "                                                                                     TNR_race_0  \\\n",
       "dataset     transform    model                                              repeat                \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410     0.661871   \n",
       "                         GPyT_in_False                                      0-2410     0.693046   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410     0.649880   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410     0.671463   \n",
       "                         Logistic Regression                                0-2410     0.697842   \n",
       "                         SVM                                                0-2410     0.752998   \n",
       "                         Agarwal LR                                         0-2410     0.724221   \n",
       "                         Kamiran & Calders LR                               0-2410     0.717026   \n",
       "                         GPyT_in_True                                       1-4820     0.721550   \n",
       "                         GPyT_in_False                                      1-4820     0.694915   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820     0.673123   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820     0.653753   \n",
       "                         Logistic Regression                                1-4820     0.748184   \n",
       "                         SVM                                                1-4820     0.762712   \n",
       "                         Agarwal LR                                         1-4820     0.753027   \n",
       "                         Kamiran & Calders LR                               1-4820     0.760291   \n",
       "                         GPyT_in_True                                       2-7230     0.689573   \n",
       "                         GPyT_in_False                                      2-7230     0.706161   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230     0.675355   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230     0.715640   \n",
       "                         Logistic Regression                                2-7230     0.722749   \n",
       "                         SVM                                                2-7230     0.736967   \n",
       "                         Agarwal LR                                         2-7230     0.755924   \n",
       "                         Kamiran & Calders LR                               2-7230     0.736967   \n",
       "                         GPyT_in_True                                       3-9640     0.688564   \n",
       "                         GPyT_in_False                                      3-9640     0.749392   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640     0.666667   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640     0.720195   \n",
       "                         Logistic Regression                                3-9640     0.749392   \n",
       "                         SVM                                                3-9640     0.742092   \n",
       "                         Agarwal LR                                         3-9640     0.751825   \n",
       "                         Kamiran & Calders LR                               3-9640     0.749392   \n",
       "                         GPyT_in_True                                       4-12050    0.694712   \n",
       "                         GPyT_in_False                                      4-12050    0.711538   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050    0.713942   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050    0.735577   \n",
       "                         Logistic Regression                                4-12050    0.730769   \n",
       "                         SVM                                                4-12050    0.745192   \n",
       "                         Agarwal LR                                         4-12050    0.737981   \n",
       "                         Kamiran & Calders LR                               4-12050    0.733173   \n",
       "\n",
       "                                                                                     TNR_race_0-race_1  \\\n",
       "dataset     transform    model                                              repeat                       \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410            0.145537   \n",
       "                         GPyT_in_False                                      0-2410            0.099547   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410            0.046416   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410            0.102611   \n",
       "                         Logistic Regression                                0-2410            0.116973   \n",
       "                         SVM                                                0-2410            0.091447   \n",
       "                         Agarwal LR                                         0-2410            0.094298   \n",
       "                         Kamiran & Calders LR                               0-2410            0.101492   \n",
       "                         GPyT_in_True                                       1-4820            0.143756   \n",
       "                         GPyT_in_False                                      1-4820            0.105085   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820            0.041162   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820            0.121757   \n",
       "                         Logistic Regression                                1-4820            0.072224   \n",
       "                         SVM                                                1-4820            0.102594   \n",
       "                         Agarwal LR                                         1-4820            0.022484   \n",
       "                         Kamiran & Calders LR                               1-4820            0.064199   \n",
       "                         GPyT_in_True                                       2-7230            0.155615   \n",
       "                         GPyT_in_False                                      2-7230            0.101370   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230            0.069414   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230            0.116996   \n",
       "                         Logistic Regression                                2-7230            0.076414   \n",
       "                         SVM                                                2-7230            0.129142   \n",
       "                         Agarwal LR                                         2-7230            0.039055   \n",
       "                         Kamiran & Calders LR                               2-7230            0.062196   \n",
       "                         GPyT_in_True                                       3-9640            0.175072   \n",
       "                         GPyT_in_False                                      3-9640            0.114245   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640            0.064738   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640            0.139309   \n",
       "                         Logistic Regression                                3-9640            0.085319   \n",
       "                         SVM                                                3-9640            0.129808   \n",
       "                         Agarwal LR                                         3-9640            0.091150   \n",
       "                         Kamiran & Calders LR                               3-9640            0.089451   \n",
       "                         GPyT_in_True                                       4-12050           0.103850   \n",
       "                         GPyT_in_False                                      4-12050           0.083426   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050           0.001712   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050           0.109747   \n",
       "                         Logistic Regression                                4-12050           0.067792   \n",
       "                         SVM                                                4-12050           0.107326   \n",
       "                         Agarwal LR                                         4-12050           0.046192   \n",
       "                         Kamiran & Calders LR                               4-12050           0.065388   \n",
       "\n",
       "                                                                                     TNR_race_0/race_1  \\\n",
       "dataset     transform    model                                              repeat                       \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410            0.819748   \n",
       "                         GPyT_in_False                                      0-2410            0.874403   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410            0.933338   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410            0.867440   \n",
       "                         Logistic Regression                                0-2410            0.856442   \n",
       "                         SVM                                                0-2410            0.891708   \n",
       "                         Agarwal LR                                         0-2410            0.884794   \n",
       "                         Kamiran & Calders LR                               0-2410            0.876005   \n",
       "                         GPyT_in_True                                       1-4820            0.833866   \n",
       "                         GPyT_in_False                                      1-4820            0.868644   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820            0.942373   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820            0.842997   \n",
       "                         Logistic Regression                                1-4820            0.911966   \n",
       "                         SVM                                                1-4820            0.881436   \n",
       "                         Agarwal LR                                         1-4820            0.971008   \n",
       "                         Kamiran & Calders LR                               1-4820            0.922135   \n",
       "                         GPyT_in_True                                       2-7230            0.815881   \n",
       "                         GPyT_in_False                                      2-7230            0.874469   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230            0.906797   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230            0.859487   \n",
       "                         Logistic Regression                                2-7230            0.904382   \n",
       "                         SVM                                                2-7230            0.850894   \n",
       "                         Agarwal LR                                         2-7230            0.950873   \n",
       "                         Kamiran & Calders LR                               2-7230            0.922173   \n",
       "                         GPyT_in_True                                       3-9640            0.797285   \n",
       "                         GPyT_in_False                                      3-9640            0.867717   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640            0.911488   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640            0.837919   \n",
       "                         Logistic Regression                                3-9640            0.897786   \n",
       "                         SVM                                                3-9640            0.851120   \n",
       "                         Agarwal LR                                         3-9640            0.891871   \n",
       "                         Kamiran & Calders LR                               3-9640            0.893364   \n",
       "                         GPyT_in_True                                       4-12050           0.869954   \n",
       "                         GPyT_in_False                                      4-12050           0.895057   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050           0.997602   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050           0.870172   \n",
       "                         Logistic Regression                                4-12050           0.915107   \n",
       "                         SVM                                                4-12050           0.874107   \n",
       "                         Agarwal LR                                         4-12050           0.941095   \n",
       "                         Kamiran & Calders LR                               4-12050           0.918118   \n",
       "\n",
       "                                                                                     TNR_race_1  \\\n",
       "dataset     transform    model                                              repeat                \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410     0.807407   \n",
       "                         GPyT_in_False                                      0-2410     0.792593   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410     0.696296   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410     0.774074   \n",
       "                         Logistic Regression                                0-2410     0.814815   \n",
       "                         SVM                                                0-2410     0.844444   \n",
       "                         Agarwal LR                                         0-2410     0.818519   \n",
       "                         Kamiran & Calders LR                               0-2410     0.818519   \n",
       "                         GPyT_in_True                                       1-4820     0.865306   \n",
       "                         GPyT_in_False                                      1-4820     0.800000   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820     0.714286   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820     0.775510   \n",
       "                         Logistic Regression                                1-4820     0.820408   \n",
       "                         SVM                                                1-4820     0.865306   \n",
       "                         Agarwal LR                                         1-4820     0.775510   \n",
       "                         Kamiran & Calders LR                               1-4820     0.824490   \n",
       "                         GPyT_in_True                                       2-7230     0.845188   \n",
       "                         GPyT_in_False                                      2-7230     0.807531   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230     0.744770   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230     0.832636   \n",
       "                         Logistic Regression                                2-7230     0.799163   \n",
       "                         SVM                                                2-7230     0.866109   \n",
       "                         Agarwal LR                                         2-7230     0.794979   \n",
       "                         Kamiran & Calders LR                               2-7230     0.799163   \n",
       "                         GPyT_in_True                                       3-9640     0.863636   \n",
       "                         GPyT_in_False                                      3-9640     0.863636   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640     0.731405   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640     0.859504   \n",
       "                         Logistic Regression                                3-9640     0.834711   \n",
       "                         SVM                                                3-9640     0.871901   \n",
       "                         Agarwal LR                                         3-9640     0.842975   \n",
       "                         Kamiran & Calders LR                               3-9640     0.838843   \n",
       "                         GPyT_in_True                                       4-12050    0.798561   \n",
       "                         GPyT_in_False                                      4-12050    0.794964   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050    0.712230   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050    0.845324   \n",
       "                         Logistic Regression                                4-12050    0.798561   \n",
       "                         SVM                                                4-12050    0.852518   \n",
       "                         Agarwal LR                                         4-12050    0.784173   \n",
       "                         Kamiran & Calders LR                               4-12050    0.798561   \n",
       "\n",
       "                                                                                          TPR  \\\n",
       "dataset     transform    model                                              repeat              \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410   0.628885   \n",
       "                         GPyT_in_False                                      0-2410   0.614260   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410   0.672761   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410   0.645338   \n",
       "                         Logistic Regression                                0-2410   0.590494   \n",
       "                         SVM                                                0-2410   0.559415   \n",
       "                         Agarwal LR                                         0-2410   0.561243   \n",
       "                         Kamiran & Calders LR                               0-2410   0.575868   \n",
       "                         GPyT_in_True                                       1-4820   0.586806   \n",
       "                         GPyT_in_False                                      1-4820   0.645833   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820   0.664931   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820   0.666667   \n",
       "                         Logistic Regression                                1-4820   0.576389   \n",
       "                         SVM                                                1-4820   0.545139   \n",
       "                         Agarwal LR                                         1-4820   0.532986   \n",
       "                         Kamiran & Calders LR                               1-4820   0.564236   \n",
       "                         GPyT_in_True                                       2-7230   0.610820   \n",
       "                         GPyT_in_False                                      2-7230   0.623037   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230   0.656195   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230   0.577661   \n",
       "                         Logistic Regression                                2-7230   0.574171   \n",
       "                         SVM                                                2-7230   0.546248   \n",
       "                         Agarwal LR                                         2-7230   0.549738   \n",
       "                         Kamiran & Calders LR                               2-7230   0.556719   \n",
       "                         GPyT_in_True                                       3-9640   0.641997   \n",
       "                         GPyT_in_False                                      3-9640   0.593804   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640   0.690189   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640   0.612737   \n",
       "                         Logistic Regression                                3-9640   0.602410   \n",
       "                         SVM                                                3-9640   0.576592   \n",
       "                         Agarwal LR                                         3-9640   0.573150   \n",
       "                         Kamiran & Calders LR                               3-9640   0.581756   \n",
       "                         GPyT_in_True                                       4-12050  0.635185   \n",
       "                         GPyT_in_False                                      4-12050  0.637037   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050  0.648148   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050  0.598148   \n",
       "                         Logistic Regression                                4-12050  0.575926   \n",
       "                         SVM                                                4-12050  0.564815   \n",
       "                         Agarwal LR                                         4-12050  0.568519   \n",
       "                         Kamiran & Calders LR                               4-12050  0.561111   \n",
       "\n",
       "                                                                                     TPR_race_0  \\\n",
       "dataset     transform    model                                              repeat                \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410     0.676166   \n",
       "                         GPyT_in_False                                      0-2410     0.645078   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410     0.678756   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410     0.683938   \n",
       "                         Logistic Regression                                0-2410     0.626943   \n",
       "                         SVM                                                0-2410     0.608808   \n",
       "                         Agarwal LR                                         0-2410     0.598446   \n",
       "                         Kamiran & Calders LR                               0-2410     0.608808   \n",
       "                         GPyT_in_True                                       1-4820     0.641361   \n",
       "                         GPyT_in_False                                      1-4820     0.685864   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820     0.680628   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820     0.714660   \n",
       "                         Logistic Regression                                1-4820     0.617801   \n",
       "                         SVM                                                1-4820     0.591623   \n",
       "                         Agarwal LR                                         1-4820     0.565445   \n",
       "                         Kamiran & Calders LR                               1-4820     0.604712   \n",
       "                         GPyT_in_True                                       2-7230     0.683417   \n",
       "                         GPyT_in_False                                      2-7230     0.675879   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230     0.693467   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230     0.630653   \n",
       "                         Logistic Regression                                2-7230     0.635678   \n",
       "                         SVM                                                2-7230     0.603015   \n",
       "                         Agarwal LR                                         2-7230     0.608040   \n",
       "                         Kamiran & Calders LR                               2-7230     0.615578   \n",
       "                         GPyT_in_True                                       3-9640     0.719340   \n",
       "                         GPyT_in_False                                      3-9640     0.658019   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640     0.735849   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640     0.676887   \n",
       "                         Logistic Regression                                3-9640     0.660377   \n",
       "                         SVM                                                3-9640     0.646226   \n",
       "                         Agarwal LR                                         3-9640     0.629717   \n",
       "                         Kamiran & Calders LR                               3-9640     0.636792   \n",
       "                         GPyT_in_True                                       4-12050    0.673469   \n",
       "                         GPyT_in_False                                      4-12050    0.673469   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050    0.647959   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050    0.630102   \n",
       "                         Logistic Regression                                4-12050    0.619898   \n",
       "                         SVM                                                4-12050    0.599490   \n",
       "                         Agarwal LR                                         4-12050    0.607143   \n",
       "                         Kamiran & Calders LR                               4-12050    0.602041   \n",
       "\n",
       "                                                                                     TPR_race_0-race_1  \\\n",
       "dataset     transform    model                                              repeat                       \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410            0.160638   \n",
       "                         GPyT_in_False                                      0-2410            0.104705   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410            0.020371   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410            0.131143   \n",
       "                         Logistic Regression                                0-2410            0.123837   \n",
       "                         SVM                                                0-2410            0.167815   \n",
       "                         Agarwal LR                                         0-2410            0.126396   \n",
       "                         Kamiran & Calders LR                               0-2410            0.111914   \n",
       "                         GPyT_in_True                                       1-4820            0.161980   \n",
       "                         GPyT_in_False                                      1-4820            0.118854   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820            0.046608   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820            0.142495   \n",
       "                         Logistic Regression                                1-4820            0.122956   \n",
       "                         SVM                                                1-4820            0.138015   \n",
       "                         Agarwal LR                                         1-4820            0.096373   \n",
       "                         Kamiran & Calders LR                               1-4820            0.120176   \n",
       "                         GPyT_in_True                                       2-7230            0.237703   \n",
       "                         GPyT_in_False                                      2-7230            0.173022   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230            0.122039   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230            0.173510   \n",
       "                         Logistic Regression                                2-7230            0.201393   \n",
       "                         SVM                                                2-7230            0.185872   \n",
       "                         Agarwal LR                                         2-7230            0.190897   \n",
       "                         Kamiran & Calders LR                               2-7230            0.192721   \n",
       "                         GPyT_in_True                                       3-9640            0.286219   \n",
       "                         GPyT_in_False                                      3-9640            0.237637   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640            0.168970   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640            0.237396   \n",
       "                         Logistic Regression                                3-9640            0.214517   \n",
       "                         SVM                                                3-9640            0.257691   \n",
       "                         Agarwal LR                                         3-9640            0.209335   \n",
       "                         Kamiran & Calders LR                               3-9640            0.203671   \n",
       "                         GPyT_in_True                                       4-12050           0.139686   \n",
       "                         GPyT_in_False                                      4-12050           0.132929   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050           0.000689   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050           0.116589   \n",
       "                         Logistic Regression                                4-12050           0.160438   \n",
       "                         SVM                                                4-12050           0.126517   \n",
       "                         Agarwal LR                                         4-12050           0.140927   \n",
       "                         Kamiran & Calders LR                               4-12050           0.149338   \n",
       "\n",
       "                                                                                     TPR_race_0/race_1  \\\n",
       "dataset     transform    model                                              repeat                       \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410            0.762428   \n",
       "                         GPyT_in_False                                      0-2410            0.837686   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410            0.969987   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410            0.808253   \n",
       "                         Logistic Regression                                0-2410            0.802474   \n",
       "                         SVM                                                0-2410            0.724356   \n",
       "                         Agarwal LR                                         0-2410            0.788793   \n",
       "                         Kamiran & Calders LR                               0-2410            0.816175   \n",
       "                         GPyT_in_True                                       1-4820            0.747444   \n",
       "                         GPyT_in_False                                      1-4820            0.826710   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820            0.931523   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820            0.800612   \n",
       "                         Logistic Regression                                1-4820            0.800979   \n",
       "                         SVM                                                1-4820            0.766718   \n",
       "                         Agarwal LR                                         1-4820            0.829563   \n",
       "                         Kamiran & Calders LR                               1-4820            0.801267   \n",
       "                         GPyT_in_True                                       2-7230            0.652185   \n",
       "                         GPyT_in_False                                      2-7230            0.744004   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230            0.824017   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230            0.724872   \n",
       "                         Logistic Regression                                2-7230            0.683185   \n",
       "                         SVM                                                2-7230            0.691762   \n",
       "                         Agarwal LR                                         2-7230            0.686045   \n",
       "                         Kamiran & Calders LR                               2-7230            0.686927   \n",
       "                         GPyT_in_True                                       3-9640            0.602109   \n",
       "                         GPyT_in_False                                      3-9640            0.638860   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640            0.770374   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640            0.649282   \n",
       "                         Logistic Regression                                3-9640            0.675159   \n",
       "                         SVM                                                3-9640            0.601237   \n",
       "                         Agarwal LR                                         3-9640            0.667573   \n",
       "                         Kamiran & Calders LR                               3-9640            0.680160   \n",
       "                         GPyT_in_True                                       4-12050           0.792588   \n",
       "                         GPyT_in_False                                      4-12050           0.802621   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050           0.998937   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050           0.814969   \n",
       "                         Logistic Regression                                4-12050           0.741186   \n",
       "                         SVM                                                4-12050           0.788959   \n",
       "                         Agarwal LR                                         4-12050           0.767886   \n",
       "                         Kamiran & Calders LR                               4-12050           0.751947   \n",
       "\n",
       "                                                                                     TPR_race_1  \\\n",
       "dataset     transform    model                                              repeat                \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410     0.515528   \n",
       "                         GPyT_in_False                                      0-2410     0.540373   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410     0.658385   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410     0.552795   \n",
       "                         Logistic Regression                                0-2410     0.503106   \n",
       "                         SVM                                                0-2410     0.440994   \n",
       "                         Agarwal LR                                         0-2410     0.472050   \n",
       "                         Kamiran & Calders LR                               0-2410     0.496894   \n",
       "                         GPyT_in_True                                       1-4820     0.479381   \n",
       "                         GPyT_in_False                                      1-4820     0.567010   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820     0.634021   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820     0.572165   \n",
       "                         Logistic Regression                                1-4820     0.494845   \n",
       "                         SVM                                                1-4820     0.453608   \n",
       "                         Agarwal LR                                         1-4820     0.469072   \n",
       "                         Kamiran & Calders LR                               1-4820     0.484536   \n",
       "                         GPyT_in_True                                       2-7230     0.445714   \n",
       "                         GPyT_in_False                                      2-7230     0.502857   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230     0.571429   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230     0.457143   \n",
       "                         Logistic Regression                                2-7230     0.434286   \n",
       "                         SVM                                                2-7230     0.417143   \n",
       "                         Agarwal LR                                         2-7230     0.417143   \n",
       "                         Kamiran & Calders LR                               2-7230     0.422857   \n",
       "                         GPyT_in_True                                       3-9640     0.433121   \n",
       "                         GPyT_in_False                                      3-9640     0.420382   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640     0.566879   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640     0.439490   \n",
       "                         Logistic Regression                                3-9640     0.445860   \n",
       "                         SVM                                                3-9640     0.388535   \n",
       "                         Agarwal LR                                         3-9640     0.420382   \n",
       "                         Kamiran & Calders LR                               3-9640     0.433121   \n",
       "                         GPyT_in_True                                       4-12050    0.533784   \n",
       "                         GPyT_in_False                                      4-12050    0.540541   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050    0.648649   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050    0.513514   \n",
       "                         Logistic Regression                                4-12050    0.459459   \n",
       "                         SVM                                                4-12050    0.472973   \n",
       "                         Agarwal LR                                         4-12050    0.466216   \n",
       "                         Kamiran & Calders LR                               4-12050    0.452703   \n",
       "\n",
       "                                                                                     prob_pos  \\\n",
       "dataset     transform    model                                              repeat              \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410   0.435170   \n",
       "                         GPyT_in_False                                      0-2410   0.421394   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410   0.482982   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410   0.446515   \n",
       "                         Logistic Regression                                0-2410   0.404376   \n",
       "                         SVM                                                0-2410   0.365478   \n",
       "                         Agarwal LR                                         0-2410   0.381686   \n",
       "                         Kamiran & Calders LR                               0-2410   0.390600   \n",
       "                         GPyT_in_True                                       1-4820   0.393841   \n",
       "                         GPyT_in_False                                      1-4820   0.443274   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820   0.476499   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820   0.471637   \n",
       "                         Logistic Regression                                1-4820   0.388979   \n",
       "                         SVM                                                1-4820   0.360616   \n",
       "                         Agarwal LR                                         1-4820   0.376013   \n",
       "                         Kamiran & Calders LR                               1-4820   0.378444   \n",
       "                         GPyT_in_True                                       2-7230   0.419773   \n",
       "                         GPyT_in_False                                      2-7230   0.427066   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230   0.465154   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230   0.397893   \n",
       "                         Logistic Regression                                2-7230   0.400324   \n",
       "                         SVM                                                2-7230   0.369530   \n",
       "                         Agarwal LR                                         2-7230   0.378444   \n",
       "                         Kamiran & Calders LR                               2-7230   0.387358   \n",
       "                         GPyT_in_True                                       3-9640   0.432739   \n",
       "                         GPyT_in_False                                      3-9640   0.389789   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640   0.488655   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640   0.409238   \n",
       "                         Logistic Regression                                3-9640   0.399514   \n",
       "                         SVM                                                3-9640   0.382496   \n",
       "                         Agarwal LR                                         3-9640   0.383306   \n",
       "                         Kamiran & Calders LR                               3-9640   0.388979   \n",
       "                         GPyT_in_True                                       4-12050  0.426256   \n",
       "                         GPyT_in_False                                      4-12050  0.422204   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050  0.444895   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050  0.385737   \n",
       "                         Logistic Regression                                4-12050  0.388169   \n",
       "                         SVM                                                4-12050  0.366288   \n",
       "                         Agarwal LR                                         4-12050  0.385737   \n",
       "                         Kamiran & Calders LR                               4-12050  0.380875   \n",
       "\n",
       "                                                                                     prob_pos_race_0  \\\n",
       "dataset     transform    model                                              repeat                     \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410          0.500623   \n",
       "                         GPyT_in_False                                      0-2410          0.469489   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410          0.508095   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410          0.499377   \n",
       "                         Logistic Regression                                0-2410          0.458281   \n",
       "                         SVM                                                0-2410          0.420922   \n",
       "                         Agarwal LR                                         0-2410          0.430884   \n",
       "                         Kamiran & Calders LR                               0-2410          0.439601   \n",
       "                         GPyT_in_True                                       1-4820          0.452830   \n",
       "                         GPyT_in_False                                      1-4820          0.488050   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820          0.496855   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820          0.523270   \n",
       "                         Logistic Regression                                1-4820          0.427673   \n",
       "                         SVM                                                1-4820          0.407547   \n",
       "                         Agarwal LR                                         1-4820          0.400000   \n",
       "                         Kamiran & Calders LR                               1-4820          0.415094   \n",
       "                         GPyT_in_True                                       2-7230          0.491463   \n",
       "                         GPyT_in_False                                      2-7230          0.479268   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230          0.503659   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230          0.452439   \n",
       "                         Logistic Regression                                2-7230          0.451220   \n",
       "                         SVM                                                2-7230          0.428049   \n",
       "                         Agarwal LR                                         2-7230          0.420732   \n",
       "                         Kamiran & Calders LR                               2-7230          0.434146   \n",
       "                         GPyT_in_True                                       3-9640          0.518563   \n",
       "                         GPyT_in_False                                      3-9640          0.457485   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640          0.537725   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640          0.481437   \n",
       "                         Logistic Regression                                3-9640          0.458683   \n",
       "                         SVM                                                3-9640          0.455090   \n",
       "                         Agarwal LR                                         3-9640          0.441916   \n",
       "                         Kamiran & Calders LR                               3-9640          0.446707   \n",
       "                         GPyT_in_True                                       4-12050         0.483911   \n",
       "                         GPyT_in_False                                      4-12050         0.475248   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050         0.461634   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050         0.441832   \n",
       "                         Logistic Regression                                4-12050         0.439356   \n",
       "                         SVM                                                4-12050         0.422030   \n",
       "                         Agarwal LR                                         4-12050         0.429455   \n",
       "                         Kamiran & Calders LR                               4-12050         0.429455   \n",
       "\n",
       "                                                                                     prob_pos_race_0-race_1  \\\n",
       "dataset     transform    model                                              repeat                            \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410                 0.187398   \n",
       "                         GPyT_in_False                                      0-2410                 0.137703   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410                 0.071900   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410                 0.151349   \n",
       "                         Logistic Regression                                0-2410                 0.154337   \n",
       "                         SVM                                                0-2410                 0.158741   \n",
       "                         Agarwal LR                                         0-2410                 0.140861   \n",
       "                         Kamiran & Calders LR                               0-2410                 0.140298   \n",
       "                         GPyT_in_True                                       1-4820                 0.165814   \n",
       "                         GPyT_in_False                                      1-4820                 0.125864   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820                 0.057220   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820                 0.145138   \n",
       "                         Logistic Regression                                1-4820                 0.108766   \n",
       "                         SVM                                                1-4820                 0.131921   \n",
       "                         Agarwal LR                                         1-4820                 0.067426   \n",
       "                         Kamiran & Calders LR                               1-4820                 0.103021   \n",
       "                         GPyT_in_True                                       2-7230                 0.213686   \n",
       "                         GPyT_in_False                                      2-7230                 0.155597   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230                 0.114770   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230                 0.162584   \n",
       "                         Logistic Regression                                2-7230                 0.151703   \n",
       "                         SVM                                                2-7230                 0.174426   \n",
       "                         Agarwal LR                                         2-7230                 0.126046   \n",
       "                         Kamiran & Calders LR                               2-7230                 0.139460   \n",
       "                         GPyT_in_True                                       3-9640                 0.265430   \n",
       "                         GPyT_in_False                                      3-9640                 0.209365   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640                 0.151760   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640                 0.223292   \n",
       "                         Logistic Regression                                3-9640                 0.182993   \n",
       "                         SVM                                                3-9640                 0.224513   \n",
       "                         Agarwal LR                                         3-9640                 0.181265   \n",
       "                         Kamiran & Calders LR                               3-9640                 0.178536   \n",
       "                         GPyT_in_True                                       4-12050                0.167009   \n",
       "                         GPyT_in_False                                      4-12050                0.153651   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050                0.048488   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050                0.162489   \n",
       "                         Logistic Regression                                4-12050                0.148277   \n",
       "                         SVM                                                4-12050                0.161466   \n",
       "                         Agarwal LR                                         4-12050                0.126639   \n",
       "                         Kamiran & Calders LR                               4-12050                0.140723   \n",
       "\n",
       "                                                                                     prob_pos_race_0/race_1  \\\n",
       "dataset     transform    model                                              repeat                            \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410                 0.625671   \n",
       "                         GPyT_in_False                                      0-2410                 0.706697   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410                 0.858491   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410                 0.696924   \n",
       "                         Logistic Regression                                0-2410                 0.663226   \n",
       "                         SVM                                                0-2410                 0.622874   \n",
       "                         Agarwal LR                                         0-2410                 0.673089   \n",
       "                         Kamiran & Calders LR                               0-2410                 0.680853   \n",
       "                         GPyT_in_True                                       1-4820                 0.633827   \n",
       "                         GPyT_in_False                                      1-4820                 0.742110   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820                 0.884836   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820                 0.722632   \n",
       "                         Logistic Regression                                1-4820                 0.745679   \n",
       "                         SVM                                                1-4820                 0.676306   \n",
       "                         Agarwal LR                                         1-4820                 0.831435   \n",
       "                         Kamiran & Calders LR                               1-4820                 0.751812   \n",
       "                         GPyT_in_True                                       2-7230                 0.565205   \n",
       "                         GPyT_in_False                                      2-7230                 0.675345   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230                 0.772128   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230                 0.640650   \n",
       "                         Logistic Regression                                2-7230                 0.663794   \n",
       "                         SVM                                                2-7230                 0.592510   \n",
       "                         Agarwal LR                                         2-7230                 0.700413   \n",
       "                         Kamiran & Calders LR                               2-7230                 0.678771   \n",
       "                         GPyT_in_True                                       3-9640                 0.488143   \n",
       "                         GPyT_in_False                                      3-9640                 0.542357   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640                 0.717774   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640                 0.536197   \n",
       "                         Logistic Regression                                3-9640                 0.601046   \n",
       "                         SVM                                                3-9640                 0.506661   \n",
       "                         Agarwal LR                                         3-9640                 0.589821   \n",
       "                         Kamiran & Calders LR                               3-9640                 0.600328   \n",
       "                         GPyT_in_True                                       4-12050                0.654876   \n",
       "                         GPyT_in_False                                      4-12050                0.676692   \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050                0.894964   \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050                0.632238   \n",
       "                         Logistic Regression                                4-12050                0.662514   \n",
       "                         SVM                                                4-12050                0.617405   \n",
       "                         Agarwal LR                                         4-12050                0.705118   \n",
       "                         Kamiran & Calders LR                               4-12050                0.672322   \n",
       "\n",
       "                                                                                     prob_pos_race_1  \n",
       "dataset     transform    model                                              repeat                    \n",
       "Compas Race no_transform GPyT_in_True                                       0-2410          0.313225  \n",
       "                         GPyT_in_False                                      0-2410          0.331787  \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 0-2410          0.436195  \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 0-2410          0.348028  \n",
       "                         Logistic Regression                                0-2410          0.303944  \n",
       "                         SVM                                                0-2410          0.262181  \n",
       "                         Agarwal LR                                         0-2410          0.290023  \n",
       "                         Kamiran & Calders LR                               0-2410          0.299304  \n",
       "                         GPyT_in_True                                       1-4820          0.287016  \n",
       "                         GPyT_in_False                                      1-4820          0.362187  \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 1-4820          0.439636  \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 1-4820          0.378132  \n",
       "                         Logistic Regression                                1-4820          0.318907  \n",
       "                         SVM                                                1-4820          0.275626  \n",
       "                         Agarwal LR                                         1-4820          0.332574  \n",
       "                         Kamiran & Calders LR                               1-4820          0.312073  \n",
       "                         GPyT_in_True                                       2-7230          0.277778  \n",
       "                         GPyT_in_False                                      2-7230          0.323671  \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 2-7230          0.388889  \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 2-7230          0.289855  \n",
       "                         Logistic Regression                                2-7230          0.299517  \n",
       "                         SVM                                                2-7230          0.253623  \n",
       "                         Agarwal LR                                         2-7230          0.294686  \n",
       "                         Kamiran & Calders LR                               2-7230          0.294686  \n",
       "                         GPyT_in_True                                       3-9640          0.253133  \n",
       "                         GPyT_in_False                                      3-9640          0.248120  \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 3-9640          0.385965  \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 3-9640          0.258145  \n",
       "                         Logistic Regression                                3-9640          0.275689  \n",
       "                         SVM                                                3-9640          0.230576  \n",
       "                         Agarwal LR                                         3-9640          0.260652  \n",
       "                         Kamiran & Calders LR                               3-9640          0.268170  \n",
       "                         GPyT_in_True                                       4-12050         0.316901  \n",
       "                         GPyT_in_False                                      4-12050         0.321596  \n",
       "                         GPyT_eq_odds_in_True_0tnr_0.71_1tnr_0.71_0tpr_0... 4-12050         0.413146  \n",
       "                         GPyT_eq_odds_in_False_0tnr_0.74_1tnr_0.74_0tpr_... 4-12050         0.279343  \n",
       "                         Logistic Regression                                4-12050         0.291080  \n",
       "                         SVM                                                4-12050         0.260563  \n",
       "                         Agarwal LR                                         4-12050         0.302817  \n",
       "                         Kamiran & Calders LR                               4-12050         0.288732  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_models(\n",
    "    datasets=data,\n",
    "    inprocess_models=algos,\n",
    "    metrics=[Accuracy(), ProbPos(), TPR(), TNR()],\n",
    "    per_sens_metrics=[ProbPos(), TPR(), TNR()], \n",
    "    repeats=5,\n",
    "    delete_prev=True,  # delete previous results\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_std_box(results, Accuracy(), TPR())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(load_data(Adult()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_0_model_Logistic Regression, C=1_completed\n",
      "fold_0_model_Logistic Regression, C=0.1_completed\n",
      "fold_0_model_Logistic Regression, C=0.01_completed\n",
      "fold_0_model_Logistic Regression, C=0.001_completed\n",
      "fold_0_model_Logistic Regression, C=0.0001_completed\n",
      "fold_0_model_Logistic Regression, C=1e-05_completed\n",
      "fold_0_model_Logistic Regression, C=1e-06_completed\n",
      "fold_0_model_Logistic Regression, C=1e-07_completed\n",
      "fold_1_model_Logistic Regression, C=1_completed\n",
      "fold_1_model_Logistic Regression, C=0.1_completed\n",
      "fold_1_model_Logistic Regression, C=0.01_completed\n",
      "fold_1_model_Logistic Regression, C=0.001_completed\n",
      "fold_1_model_Logistic Regression, C=0.0001_completed\n",
      "fold_1_model_Logistic Regression, C=1e-05_completed\n",
      "fold_1_model_Logistic Regression, C=1e-06_completed\n",
      "fold_1_model_Logistic Regression, C=1e-07_completed\n",
      "fold_2_model_Logistic Regression, C=1_completed\n",
      "fold_2_model_Logistic Regression, C=0.1_completed\n",
      "fold_2_model_Logistic Regression, C=0.01_completed\n",
      "fold_2_model_Logistic Regression, C=0.001_completed\n",
      "fold_2_model_Logistic Regression, C=0.0001_completed\n",
      "fold_2_model_Logistic Regression, C=1e-05_completed\n",
      "fold_2_model_Logistic Regression, C=1e-06_completed\n",
      "fold_2_model_Logistic Regression, C=1e-07_completed\n",
      "fold_3_model_Logistic Regression, C=1_completed\n",
      "fold_3_model_Logistic Regression, C=0.1_completed\n",
      "fold_3_model_Logistic Regression, C=0.01_completed\n",
      "fold_3_model_Logistic Regression, C=0.001_completed\n",
      "fold_3_model_Logistic Regression, C=0.0001_completed\n",
      "fold_3_model_Logistic Regression, C=1e-05_completed\n",
      "fold_3_model_Logistic Regression, C=1e-06_completed\n",
      "fold_3_model_Logistic Regression, C=1e-07_completed\n",
      "fold_4_model_Logistic Regression, C=1_completed\n",
      "fold_4_model_Logistic Regression, C=0.1_completed\n",
      "fold_4_model_Logistic Regression, C=0.01_completed\n",
      "fold_4_model_Logistic Regression, C=0.001_completed\n",
      "fold_4_model_Logistic Regression, C=0.0001_completed\n",
      "fold_4_model_Logistic Regression, C=1e-05_completed\n",
      "fold_4_model_Logistic Regression, C=1e-06_completed\n",
      "fold_4_model_Logistic Regression, C=1e-07_completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fair grid search\n",
    "primary = Accuracy()\n",
    "fair_measure = AbsCV()\n",
    "hyperparams = dict(C=[1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7])\n",
    "lr_cv = CrossValidator(LR, hyperparams, folds=5)\n",
    "lr_cv.run(train, measures=[primary, fair_measure])\n",
    "lr_cv.best_hyper_params(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: ResultTuple(params={'C': 0.1}, fold_id=-1, scores={'Accuracy': 0.8460072554374714, 'CV absolute': 0.825007917407934})\n",
      "best fair(+accuracy): ResultTuple(params={'C': 1e-06}, fold_id=-1, scores={'Accuracy': 0.7909169578659723, 'CV absolute': 0.9530144052067022})\n"
     ]
    }
   ],
   "source": [
    "lr_best_acc = lr_cv.results.get_best_result(primary)\n",
    "lr_best_fair = lr_cv.results.get_best_in_top_k(primary, fair_measure, top_k=3)\n",
    "print(\"best accuracy:\", lr_best_acc)\n",
    "print(\"best fair(+accuracy):\", lr_best_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.84it/s, model=Logistic Regression, C=0.1, dataset=Adult Sex, transform=no_transform, repeat=2]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TNR</th>\n",
       "      <th>TNR_sex_Male_0</th>\n",
       "      <th>TNR_sex_Male_0-sex_Male_1</th>\n",
       "      <th>TNR_sex_Male_0/sex_Male_1</th>\n",
       "      <th>TNR_sex_Male_1</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TPR_sex_Male_0</th>\n",
       "      <th>TPR_sex_Male_0-sex_Male_1</th>\n",
       "      <th>TPR_sex_Male_0/sex_Male_1</th>\n",
       "      <th>TPR_sex_Male_1</th>\n",
       "      <th>prob_pos</th>\n",
       "      <th>prob_pos_sex_Male_0</th>\n",
       "      <th>prob_pos_sex_Male_0-sex_Male_1</th>\n",
       "      <th>prob_pos_sex_Male_0/sex_Male_1</th>\n",
       "      <th>prob_pos_sex_Male_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>transform</th>\n",
       "      <th>model</th>\n",
       "      <th>repeat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Adult Sex</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">no_transform</th>\n",
       "      <th>Logistic Regression, C=1e-06</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.789829</td>\n",
       "      <td>0.962122</td>\n",
       "      <td>0.970254</td>\n",
       "      <td>0.013044</td>\n",
       "      <td>0.986556</td>\n",
       "      <td>0.957210</td>\n",
       "      <td>0.272566</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.964032</td>\n",
       "      <td>0.271134</td>\n",
       "      <td>0.096517</td>\n",
       "      <td>0.057739</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.503890</td>\n",
       "      <td>0.114587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression, C=0.1</th>\n",
       "      <th>0-2410</th>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.930140</td>\n",
       "      <td>0.974168</td>\n",
       "      <td>0.070622</td>\n",
       "      <td>0.927505</td>\n",
       "      <td>0.903546</td>\n",
       "      <td>0.616814</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.081476</td>\n",
       "      <td>0.870334</td>\n",
       "      <td>0.628351</td>\n",
       "      <td>0.206523</td>\n",
       "      <td>0.083826</td>\n",
       "      <td>0.179869</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>0.263695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression, C=1e-06</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.961874</td>\n",
       "      <td>0.967232</td>\n",
       "      <td>0.008689</td>\n",
       "      <td>0.991017</td>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.272608</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.272586</td>\n",
       "      <td>0.097181</td>\n",
       "      <td>0.060647</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.528610</td>\n",
       "      <td>0.114730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression, C=0.1</th>\n",
       "      <th>1-4820</th>\n",
       "      <td>0.843339</td>\n",
       "      <td>0.935422</td>\n",
       "      <td>0.982652</td>\n",
       "      <td>0.076589</td>\n",
       "      <td>0.922058</td>\n",
       "      <td>0.906063</td>\n",
       "      <td>0.569798</td>\n",
       "      <td>0.436950</td>\n",
       "      <td>0.156235</td>\n",
       "      <td>0.736617</td>\n",
       "      <td>0.593185</td>\n",
       "      <td>0.191819</td>\n",
       "      <td>0.066099</td>\n",
       "      <td>0.186111</td>\n",
       "      <td>0.262079</td>\n",
       "      <td>0.252209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression, C=1e-06</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.793367</td>\n",
       "      <td>0.964702</td>\n",
       "      <td>0.967868</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.994750</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>0.283201</td>\n",
       "      <td>0.290960</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.968416</td>\n",
       "      <td>0.281771</td>\n",
       "      <td>0.097623</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>0.559910</td>\n",
       "      <td>0.113699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression, C=0.1</th>\n",
       "      <th>2-7230</th>\n",
       "      <td>0.849972</td>\n",
       "      <td>0.934722</td>\n",
       "      <td>0.978056</td>\n",
       "      <td>0.069547</td>\n",
       "      <td>0.928892</td>\n",
       "      <td>0.908509</td>\n",
       "      <td>0.597625</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.135699</td>\n",
       "      <td>0.780688</td>\n",
       "      <td>0.618750</td>\n",
       "      <td>0.199116</td>\n",
       "      <td>0.078114</td>\n",
       "      <td>0.178279</td>\n",
       "      <td>0.304665</td>\n",
       "      <td>0.256394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Accuracy  \\\n",
       "dataset   transform    model                        repeat             \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410  0.789829   \n",
       "                       Logistic Regression, C=0.1   0-2410  0.851852   \n",
       "                       Logistic Regression, C=1e-06 1-4820  0.788281   \n",
       "                       Logistic Regression, C=0.1   1-4820  0.843339   \n",
       "                       Logistic Regression, C=1e-06 2-7230  0.793367   \n",
       "                       Logistic Regression, C=0.1   2-7230  0.849972   \n",
       "\n",
       "                                                                 TNR  \\\n",
       "dataset   transform    model                        repeat             \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410  0.962122   \n",
       "                       Logistic Regression, C=0.1   0-2410  0.930140   \n",
       "                       Logistic Regression, C=1e-06 1-4820  0.961874   \n",
       "                       Logistic Regression, C=0.1   1-4820  0.935422   \n",
       "                       Logistic Regression, C=1e-06 2-7230  0.964702   \n",
       "                       Logistic Regression, C=0.1   2-7230  0.934722   \n",
       "\n",
       "                                                            TNR_sex_Male_0  \\\n",
       "dataset   transform    model                        repeat                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410        0.970254   \n",
       "                       Logistic Regression, C=0.1   0-2410        0.974168   \n",
       "                       Logistic Regression, C=1e-06 1-4820        0.967232   \n",
       "                       Logistic Regression, C=0.1   1-4820        0.982652   \n",
       "                       Logistic Regression, C=1e-06 2-7230        0.967868   \n",
       "                       Logistic Regression, C=0.1   2-7230        0.978056   \n",
       "\n",
       "                                                            TNR_sex_Male_0-sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                              \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                   0.013044   \n",
       "                       Logistic Regression, C=0.1   0-2410                   0.070622   \n",
       "                       Logistic Regression, C=1e-06 1-4820                   0.008689   \n",
       "                       Logistic Regression, C=0.1   1-4820                   0.076589   \n",
       "                       Logistic Regression, C=1e-06 2-7230                   0.005081   \n",
       "                       Logistic Regression, C=0.1   2-7230                   0.069547   \n",
       "\n",
       "                                                            TNR_sex_Male_0/sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                              \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                   0.986556   \n",
       "                       Logistic Regression, C=0.1   0-2410                   0.927505   \n",
       "                       Logistic Regression, C=1e-06 1-4820                   0.991017   \n",
       "                       Logistic Regression, C=0.1   1-4820                   0.922058   \n",
       "                       Logistic Regression, C=1e-06 2-7230                   0.994750   \n",
       "                       Logistic Regression, C=0.1   2-7230                   0.928892   \n",
       "\n",
       "                                                            TNR_sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410        0.957210   \n",
       "                       Logistic Regression, C=0.1   0-2410        0.903546   \n",
       "                       Logistic Regression, C=1e-06 1-4820        0.958543   \n",
       "                       Logistic Regression, C=0.1   1-4820        0.906063   \n",
       "                       Logistic Regression, C=1e-06 2-7230        0.962787   \n",
       "                       Logistic Regression, C=0.1   2-7230        0.908509   \n",
       "\n",
       "                                                                 TPR  \\\n",
       "dataset   transform    model                        repeat             \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410  0.272566   \n",
       "                       Logistic Regression, C=0.1   0-2410  0.616814   \n",
       "                       Logistic Regression, C=1e-06 1-4820  0.272608   \n",
       "                       Logistic Regression, C=0.1   1-4820  0.569798   \n",
       "                       Logistic Regression, C=1e-06 2-7230  0.283201   \n",
       "                       Logistic Regression, C=0.1   2-7230  0.597625   \n",
       "\n",
       "                                                            TPR_sex_Male_0  \\\n",
       "dataset   transform    model                        repeat                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410        0.281250   \n",
       "                       Logistic Regression, C=0.1   0-2410        0.546875   \n",
       "                       Logistic Regression, C=1e-06 1-4820        0.272727   \n",
       "                       Logistic Regression, C=0.1   1-4820        0.436950   \n",
       "                       Logistic Regression, C=1e-06 2-7230        0.290960   \n",
       "                       Logistic Regression, C=0.1   2-7230        0.483051   \n",
       "\n",
       "                                                            TPR_sex_Male_0-sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                              \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                   0.010116   \n",
       "                       Logistic Regression, C=0.1   0-2410                   0.081476   \n",
       "                       Logistic Regression, C=1e-06 1-4820                   0.000141   \n",
       "                       Logistic Regression, C=0.1   1-4820                   0.156235   \n",
       "                       Logistic Regression, C=1e-06 2-7230                   0.009190   \n",
       "                       Logistic Regression, C=0.1   2-7230                   0.135699   \n",
       "\n",
       "                                                            TPR_sex_Male_0/sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                              \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                   0.964032   \n",
       "                       Logistic Regression, C=0.1   0-2410                   0.870334   \n",
       "                       Logistic Regression, C=1e-06 1-4820                   0.999484   \n",
       "                       Logistic Regression, C=0.1   1-4820                   0.736617   \n",
       "                       Logistic Regression, C=1e-06 2-7230                   0.968416   \n",
       "                       Logistic Regression, C=0.1   2-7230                   0.780688   \n",
       "\n",
       "                                                            TPR_sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410        0.271134   \n",
       "                       Logistic Regression, C=0.1   0-2410        0.628351   \n",
       "                       Logistic Regression, C=1e-06 1-4820        0.272586   \n",
       "                       Logistic Regression, C=0.1   1-4820        0.593185   \n",
       "                       Logistic Regression, C=1e-06 2-7230        0.281771   \n",
       "                       Logistic Regression, C=0.1   2-7230        0.618750   \n",
       "\n",
       "                                                            prob_pos  \\\n",
       "dataset   transform    model                        repeat             \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410  0.096517   \n",
       "                       Logistic Regression, C=0.1   0-2410  0.206523   \n",
       "                       Logistic Regression, C=1e-06 1-4820  0.097181   \n",
       "                       Logistic Regression, C=0.1   1-4820  0.191819   \n",
       "                       Logistic Regression, C=1e-06 2-7230  0.097623   \n",
       "                       Logistic Regression, C=0.1   2-7230  0.199116   \n",
       "\n",
       "                                                            prob_pos_sex_Male_0  \\\n",
       "dataset   transform    model                        repeat                        \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410             0.057739   \n",
       "                       Logistic Regression, C=0.1   0-2410             0.083826   \n",
       "                       Logistic Regression, C=1e-06 1-4820             0.060647   \n",
       "                       Logistic Regression, C=0.1   1-4820             0.066099   \n",
       "                       Logistic Regression, C=1e-06 2-7230             0.063661   \n",
       "                       Logistic Regression, C=0.1   2-7230             0.078114   \n",
       "\n",
       "                                                            prob_pos_sex_Male_0-sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                        0.056848   \n",
       "                       Logistic Regression, C=0.1   0-2410                        0.179869   \n",
       "                       Logistic Regression, C=1e-06 1-4820                        0.054083   \n",
       "                       Logistic Regression, C=0.1   1-4820                        0.186111   \n",
       "                       Logistic Regression, C=1e-06 2-7230                        0.050038   \n",
       "                       Logistic Regression, C=0.1   2-7230                        0.178279   \n",
       "\n",
       "                                                            prob_pos_sex_Male_0/sex_Male_1  \\\n",
       "dataset   transform    model                        repeat                                   \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410                        0.503890   \n",
       "                       Logistic Regression, C=0.1   0-2410                        0.317890   \n",
       "                       Logistic Regression, C=1e-06 1-4820                        0.528610   \n",
       "                       Logistic Regression, C=0.1   1-4820                        0.262079   \n",
       "                       Logistic Regression, C=1e-06 2-7230                        0.559910   \n",
       "                       Logistic Regression, C=0.1   2-7230                        0.304665   \n",
       "\n",
       "                                                            prob_pos_sex_Male_1  \n",
       "dataset   transform    model                        repeat                       \n",
       "Adult Sex no_transform Logistic Regression, C=1e-06 0-2410             0.114587  \n",
       "                       Logistic Regression, C=0.1   0-2410             0.263695  \n",
       "                       Logistic Regression, C=1e-06 1-4820             0.114730  \n",
       "                       Logistic Regression, C=0.1   1-4820             0.252209  \n",
       "                       Logistic Regression, C=1e-06 2-7230             0.113699  \n",
       "                       Logistic Regression, C=0.1   2-7230             0.256394  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_best_acc_model = LR(**lr_best_acc.params)\n",
    "results = evaluate_models(\n",
    "    datasets=[Adult()],\n",
    "    inprocess_models=[LR(**lr_best_fair.params), lr_best_acc_model],\n",
    "    metrics=[Accuracy(), ProbPos(), TPR(), TNR()],\n",
    "    per_sens_metrics=[ProbPos(), TPR(), TNR()], \n",
    "    repeats=3,\n",
    "    delete_prev=True,  # delete previous results\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_std_box(results, Accuracy(), ProbPos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
