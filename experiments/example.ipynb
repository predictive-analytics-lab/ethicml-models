{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethicml\n",
    "from ethicml.algorithms.inprocess import GPyT, GPyTDemPar, GPyTEqOdds, LR, SVM, Agarwal, Kamiran, Kamishima, LRCV, ZafarEqOpp\n",
    "from ethicml.evaluators import evaluate_models, CrossValidator, run_metrics\n",
    "from ethicml.data import Compas, Adult, load_data\n",
    "from ethicml.metrics import Accuracy, ProbPos, TPR, TNR, AbsCV\n",
    "from ethicml.preprocessing import train_test_split\n",
    "from ethicml.visualisation.plot import plot_mean_std_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T HAVE TO UNDERSTAND THE CODE IN THIS CELL\n",
    "# we only tell the model where the \"run.py\" is and where the python executable is\n",
    "code_dir = Path('..')\n",
    "def gp(**kwargs):\n",
    "    return GPyT(code_dir=code_dir, **kwargs)\n",
    "def gp_dp(**kwargs):\n",
    "    return GPyTDemPar(code_dir=code_dir, **kwargs)\n",
    "def gp_eo(**kwargs):\n",
    "    return GPyTEqOdds(code_dir=code_dir, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnr_race_False = 0.724\n",
    "# tnr_race_True = 0.702\n",
    "# tnr_sex_True = 0.724\n",
    "# tnr_sex_False = 0.744\n",
    "# tnr_in_true_race = 0.71\n",
    "# tnr_in_false_race = 0.74\n",
    "# tnr_in_true_sex = 0.72\n",
    "# tnr_in_false_sex = 0.77\n",
    "\n",
    "tnr_in_false_race = 0.74\n",
    "tnr_in_false_sex = 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify flags for GP\n",
    "gp_flags = dict(epochs=70, length_scale=1.2, use_loo=False, iso=False, cov='GridInterpolationKernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = []\n",
    "\n",
    "# algos += [gp(s_as_input=True, flags=gp_flags)]\n",
    "algos += [gp(s_as_input=False, flags=gp_flags)]\n",
    "# algos += [gp_dp(epochs=70, s_as_input=True)]\n",
    "\n",
    "for tpr in [0.6,0.7,0.8,0.9]:\n",
    "#     algos += [gp_eo(s_as_input=True, tnr1=tnr_in_true_race, tnr0=tnr_in_true_race, tpr0=tpr, tpr1=tpr, flags=gp_flags)]\n",
    "    algos += [gp_eo(s_as_input=False, tnr1=tnr_in_false_race, tnr0=tnr_in_false_race, tpr0=tpr, tpr1=tpr, flags=gp_flags)]\n",
    "\n",
    "baselines = [\n",
    "#      LR(),\n",
    "     SVM(),\n",
    "#     Agarwal(fairness=\"EqOd\"),\n",
    "#     Kamiran(),\n",
    "#     Kamishima(),\n",
    "#    ZafarEqOpp(),\n",
    "]\n",
    "algos += baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "#      Compas(\"Race\"),\n",
    "      Compas(\"Sex\"),\n",
    "#     Adult(\"Race\"),\n",
    "#     Adult(\"Sex\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s, model=GPyT_in_False, dataset=Compas Sex, transform=no_transform, repeat=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure: ['../run.py', '--inf', 'Variational', '--data', 'sensitive_from_numpy', '--dataset_path', '/tmp/tmpe30xc2lm/data.npz', '--cov', 'GridInterpolationKernel', '--mean', 'ZeroMean', '--optimizer', 'Adam', '--lr', '0.05', '--model_name', 'local', '--batch_size', '4933', '--epochs', '70', '--eval_epochs', '100000', '--summary_steps', '100000', '--chkpt_epochs', '100000', '--save_dir', '/tmp/tmpe30xc2lm', '--plot', '', '--logging_steps', '1', '--gpus', '0', '--preds_path', 'predictions.npz', '--num_samples', '20', '--optimize_inducing', 'True', '--length_scale', '1.2', '--sf', '1.0', '--iso', 'False', '--num_samples_pred', '2000', '--s_as_input', 'False', '--num_inducing', '1245', '--manual_seed', '888', '--lik', 'BaselineLikelihood', '--use_loo', 'False'] (pid = 25070)\n",
      "/mnt/archive/fairness/fair-gpytorch/fairgp/datasets.py:85: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.where(max_per_feature > 1e-7, unnormalized / max_per_feature, unnormalized)\n",
      "/mnt/archive/fairness/fair-gpytorch/fairgp/datasets.py:85: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.where(max_per_feature > 1e-7, unnormalized / max_per_feature, unnormalized)\n",
      "/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/kernels/kernel.py:183: UserWarning: The 'param_transform' argument is now deprecated. If you want to use a different transformation, specify a different 'lengthscale_constraint' instead.\n",
      "  warnings.warn(\"The 'param_transform' argument is now deprecated. If you want to use a different \"\n",
      "Traceback (most recent call last):\n",
      "  File \"../run.py\", line 3, in <module>\n",
      "    training.main_loop(flags.parse_arguments())\n",
      "  File \"/mnt/archive/fairness/fair-gpytorch/fairgp/training.py\", line 64, in main_loop\n",
      "    train(model, optimizer, train_loader, mll, step_counter, flags)\n",
      "  File \"/mnt/archive/fairness/fair-gpytorch/fairgp/training.py\", line 109, in train\n",
      "    output = model(inputs)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/models/abstract_variational_gp.py\", line 22, in __call__\n",
      "    return self.variational_strategy(inputs)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py\", line 223, in __call__\n",
      "    self.initialize_variational_dist()\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py\", line 90, in initialize_variational_dist\n",
      "    prior_dist = self.prior_distribution\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/utils/memoize.py\", line 34, in g\n",
      "    add_to_cache(self, cache_name, method(self, *args, **kwargs))\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py\", line 71, in prior_distribution\n",
      "    out.mean, out.lazy_covariance_matrix.add_jitter()\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 222, in add_jitter\n",
      "    return self.evaluate_kernel().add_jitter(jitter_val)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/utils/memoize.py\", line 34, in g\n",
      "    add_to_cache(self, cache_name, method(self, *args, **kwargs))\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 270, in evaluate_kernel\n",
      "    x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\", line 394, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/module.py\", line 22, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/kernels/grid_interpolation_kernel.py\", line 167, in forward\n",
      "    left_interp_indices, left_interp_values = self._compute_grid(x1, last_dim_is_batch)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/kernels/grid_interpolation_kernel.py\", line 125, in _compute_grid\n",
      "    interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)\n",
      "  File \"/mnt/archive/fairness/py_env/lib/python3.6/site-packages/gpytorch/utils/interpolation.py\", line 57, in interpolate\n",
      "    x_target_min[first_out_of_range].item(),\n",
      "IndexError: index 1 is out of bounds for dimension 0 with size 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The script failed. Supplied arguments: ['../run.py', '--inf', 'Variational', '--data', 'sensitive_from_numpy', '--dataset_path', '/tmp/tmpe30xc2lm/data.npz', '--cov', 'GridInterpolationKernel', '--mean', 'ZeroMean', '--optimizer', 'Adam', '--lr', '0.05', '--model_name', 'local', '--batch_size', '4933', '--epochs', '70', '--eval_epochs', '100000', '--summary_steps', '100000', '--chkpt_epochs', '100000', '--save_dir', '/tmp/tmpe30xc2lm', '--plot', '', '--logging_steps', '1', '--gpus', '0', '--preds_path', 'predictions.npz', '--num_samples', '20', '--optimize_inducing', 'True', '--length_scale', '1.2', '--sf', '1.0', '--iso', 'False', '--num_samples_pred', '2000', '--s_as_input', 'False', '--num_inducing', '1245', '--manual_seed', '888', '--lik', 'BaselineLikelihood', '--use_loo', 'False'] with exec: /mnt/archive/fairness/py_env/bin/python3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f8fb2f6bdede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mproportional_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# <- new!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdelete_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# delete previous results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/evaluators/evaluate_models.py\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(datasets, preprocess_models, inprocess_models, postprocess_models, metrics, per_sens_metrics, repeats, test_mode, delete_prev, proportional_splits, topic, start_seed)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mtemp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_sens_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/algorithms/inprocess/in_algorithm.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train, test)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTestTuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTestTuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/algorithms/algorithm_base.py\u001b[0m in \u001b[0;36mrun_blocking\u001b[0;34m(promise)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m\"\"\"Run an asynchronous process as a blocking process\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event loop stopped before Future completed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/algorithms/inprocess/fair_gpyt.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, train, test)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ytrain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             )\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_gpyt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Read the results from the numpy file 'predictions.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/algorithms/inprocess/fair_gpyt.py\u001b[0m in \u001b[0;36m_run_gpyt\u001b[0;34m(self, flags)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmd_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"--{key}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/archive/fairness/py_env/lib/python3.6/site-packages/ethicml/algorithms/algorithm_base.py\u001b[0m in \u001b[0;36m_call_script\u001b[0;34m(self, cmd_args, env, cwd)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise RuntimeError(\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;34mf\"The script failed. Supplied arguments: {cmd_args} with exec: {self._executable}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The script failed. Supplied arguments: ['../run.py', '--inf', 'Variational', '--data', 'sensitive_from_numpy', '--dataset_path', '/tmp/tmpe30xc2lm/data.npz', '--cov', 'GridInterpolationKernel', '--mean', 'ZeroMean', '--optimizer', 'Adam', '--lr', '0.05', '--model_name', 'local', '--batch_size', '4933', '--epochs', '70', '--eval_epochs', '100000', '--summary_steps', '100000', '--chkpt_epochs', '100000', '--save_dir', '/tmp/tmpe30xc2lm', '--plot', '', '--logging_steps', '1', '--gpus', '0', '--preds_path', 'predictions.npz', '--num_samples', '20', '--optimize_inducing', 'True', '--length_scale', '1.2', '--sf', '1.0', '--iso', 'False', '--num_samples_pred', '2000', '--s_as_input', 'False', '--num_inducing', '1245', '--manual_seed', '888', '--lik', 'BaselineLikelihood', '--use_loo', 'False'] with exec: /mnt/archive/fairness/py_env/bin/python3"
     ]
    }
   ],
   "source": [
    "results = evaluate_models(\n",
    "    datasets=data,\n",
    "    inprocess_models=algos,\n",
    "    metrics=[Accuracy(), TPR(), TNR()],\n",
    "    per_sens_metrics=[TPR(), TNR()], \n",
    "    repeats=5,\n",
    "    proportional_splits=True,                      # <- new!\n",
    "    delete_prev=True,  # delete previous results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Figure size 1800x1200 with 1 Axes>,\n",
       "  <matplotlib.axes._subplots.AxesSubplot at 0x7ff1a4ac2710>),\n",
       " (<Figure size 1800x1200 with 1 Axes>,\n",
       "  <matplotlib.axes._subplots.AxesSubplot at 0x7ff1a48b2128>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# results = pd.read_csv(\"results/Compas Sex_no_transform.csv\").set_index([\"dataset\", \"transform\", \"model\", \"repeat\"])\n",
    "\n",
    "plot_mean_std_box(results, Accuracy(), TPR())\n",
    "# figs_plots = plot_mean_std_box(results, \"Accuracy\", \"TPR_race_0/race_1\")\n",
    "# figs_plots[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(load_data(Adult()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair grid search\n",
    "primary = Accuracy()\n",
    "fair_measure = AbsCV()\n",
    "hyperparams = dict(C=[1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7])\n",
    "lr_cv = CrossValidator(LR, hyperparams, folds=5)\n",
    "lr_cv.run(train, measures=[primary, fair_measure])\n",
    "lr_cv.best_hyper_params(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_acc = lr_cv.results.get_best_result(primary)\n",
    "lr_best_fair = lr_cv.results.get_best_in_top_k(primary, fair_measure, top_k=3)\n",
    "print(\"best accuracy:\", lr_best_acc)\n",
    "print(\"best fair(+accuracy):\", lr_best_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_acc_model = LRCV()\n",
    "results = evaluate_models(\n",
    "    datasets=[Adult()],\n",
    "    inprocess_models=[LR(**lr_best_fair.params), lr_best_acc_model],\n",
    "    metrics=[Accuracy(), ProbPos(), TPR(), TNR()],\n",
    "    per_sens_metrics=[ProbPos(), TPR(), TNR()], \n",
    "    repeats=3,\n",
    "    delete_prev=True,  # delete previous results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_std_box(results, Accuracy(), ProbPos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
